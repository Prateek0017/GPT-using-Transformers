{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Coding Assignment Implementation and Optimization of GPT-2 Model \n**Objective:**\n\nThis assignment aims to test your understand  ing of the Transformer architecture, and your ability to modify its structures for improved performance. Further, it requires your skills to develop an efficient training loop and implementation of distributed training applicable across multiple GPUs.\n\n**Points:**\n\nTotal points for the assignment are **100** and are distributed as follows:\n\n- Task 1: Model Implementation and Checkpoints (20 points)\n- Task 2: Architectural changes (40 points)\n- Task 3: Distributed Training (40 points)\n\n","metadata":{}},{"cell_type":"markdown","source":"### Task 1 | GPT-2 Model & Checkpoints (20 Points)\n\n---\n\nStart by implementing the `GPT2-small` model (with 125 million parameters) using Python and PyTorch. Make sure you touch upon the key aspects of the model like multi-head self-attention mechanism, feed-forward networks and positional encoding.\n\nKey points:\n\n- Follow the original GPT-2 design of using both token and positional embeddings.\n- Implement the transformer layers with multi-head self-attention and point-wise feed-forward network.\n- You're required to abstain from using pre-built transformer libraries.\n\nRefer to the GPT-2 paper's architecture descriptions in Sections 1 and 2 for further help. ([GPT-2 paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)). Additionally, a great resource could be Andrej Karpathyâ€™s [nanogpt](https://github.com/karpathy/nanoGPT) repository and the [makemore](https://youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&feature=shared) series.\n\nTo validate your implementation, load the original GPT-2 125M model checkpoints and run a sample prediction.\n\n**Deliverable:** Complete Python code featuring the GPT-2 model along with demonstration of appropriate testing to verify its functioning","metadata":{}},{"cell_type":"markdown","source":"**Task 1 solution-**\n\nFirst we Setup\nWe import the modules. Most of it comes from torch, with the exception of the built-in math module.\n\n","metadata":{}},{"cell_type":"code","source":"import math\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2023-12-15T20:45:09.215800Z","iopub.execute_input":"2023-12-15T20:45:09.216235Z","iopub.status.idle":"2023-12-15T20:45:09.222605Z","shell.execute_reply.started":"2023-12-15T20:45:09.216204Z","shell.execute_reply":"2023-12-15T20:45:09.221192Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Declaring hyperparameters ( like maximum sequence length, and embedding dimension)","metadata":{}},{"cell_type":"code","source":"class GPTConfig:\n    attn_dropout = 0.1\n    embed_dropout = 0.1\n    ff_dropout = 0.1\n    \n    def __init__(\n        self, vocab_size, max_len, **kwargs\n    ):\n        self.vocab_size = vocab_size\n        self.max_len = max_len\n        for key, value in kwargs.items():\n            setattr(self, key, value)\n\nclass GPT1Config(GPTConfig):\n    num_heads = 12\n    num_blocks = 12\n    embed_dim = 768","metadata":{"execution":{"iopub.status.busy":"2023-12-15T20:45:09.224570Z","iopub.execute_input":"2023-12-15T20:45:09.225165Z","iopub.status.idle":"2023-12-15T20:45:09.234499Z","shell.execute_reply.started":"2023-12-15T20:45:09.225135Z","shell.execute_reply":"2023-12-15T20:45:09.233480Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"**Implementation**\n\nstart building out the model-","metadata":{}},{"cell_type":"code","source":"class GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        embed_dim = config.embed_dim\n        self.max_len = config.max_len\n        self.tok_embed = nn.Embedding(\n            config.vocab_size, embed_dim\n        )\n        self.pos_embed = nn.Parameter(\n            torch.zeros(1, config.max_len, embed_dim)\n        )\n        self.dropout = nn.Dropout(config.embed_dropout)\n        self.blocks = nn.Sequential(\n            *[Block(config) for _ in range(config.num_blocks)]\n        )\n        self.ln = nn.LayerNorm(embed_dim)\n        self.fc = nn.Linear(embed_dim, config.vocab_size)\n    \n    def forward(self, x, target=None):\n        # batch_size = x.size(0)\n        seq_len = x.size(1)\n        assert seq_len <= self.max_len, \"sequence longer than model capacity\"\n        \n        tok_embedding = self.tok_embed(x)\n        # tok_embedding.shape == (batch_size, seq_len, embed_dim)\n        pos_embedding = self.pos_embed[:, :seq_len, :]\n        # pos_embedding.shape == (1, seq_len, embed_dim)\n        x = self.dropout(tok_embedding + pos_embedding)\n        x = self.blocks(x)\n        x = self.ln(x)\n        x = self.fc(x)\n        # x.shape == (batch_size, seq_len, vocab_size)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-15T20:45:09.236376Z","iopub.execute_input":"2023-12-15T20:45:09.236729Z","iopub.status.idle":"2023-12-15T20:45:09.251332Z","shell.execute_reply.started":"2023-12-15T20:45:09.236700Z","shell.execute_reply":"2023-12-15T20:45:09.250100Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**Decoder Block**\nThe building blocks of the decoder, the transformer decoder block. A decoder block consists of multi-head attention, layer normalization, and a point-wise feedforward network. ","metadata":{}},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        embed_dim = config.embed_dim\n        self.ln1 = nn.LayerNorm(embed_dim)\n        self.ln2 = nn.LayerNorm(embed_dim)\n        self.attn = MultiheadAttention(config)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim * 4),\n            nn.GELU(),\n            nn.Linear(embed_dim * 4, embed_dim),\n            nn.Dropout(config.ff_dropout),\n        )\n    \n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))\n        x = x + self.ff(self.ln2(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-15T20:45:09.253814Z","iopub.execute_input":"2023-12-15T20:45:09.254490Z","iopub.status.idle":"2023-12-15T20:45:09.263318Z","shell.execute_reply.started":"2023-12-15T20:45:09.254451Z","shell.execute_reply":"2023-12-15T20:45:09.262133Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"Creating Class Multihead","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiheadAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        embed_dim = config.embed_dim\n        self.num_heads = config.num_heads\n        assert embed_dim % self.num_heads == 0, \"invalid heads and embedding dimension configuration\"\n        self.key = nn.Linear(embed_dim, embed_dim)\n        self.value = nn.Linear(embed_dim, embed_dim)\n        self.query = nn.Linear(embed_dim, embed_dim)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.attn_dropout = nn.Dropout(config.attn_dropout)\n        self.proj_dropout = nn.Dropout(config.ff_dropout)\n        self.register_buffer(\n            \"mask\", \n            torch.tril(torch.ones(config.max_len, config.max_len))\n            .unsqueeze(0).unsqueeze(0)\n        )\n    \n    def forward(self, x):\n        batch_size = x.size(0)\n        seq_len = x.size(1)\n        # x.shape == (batch_size, seq_len, embed_dim)\n        k_t = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n        # shape == (batch_size, num_heads, seq_len, head_dim)\n        \n        attn = torch.matmul(q, k_t) / math.sqrt(q.size(-1))\n        # attn.shape == (batch_size, num_heads, seq_len, seq_len)\n        mask = self.mask[:, :, :seq_len, :seq_len]\n        attn = attn.masked_fill(mask == 0, float(\"-inf\"))\n        attn = self.attn_dropout(attn)\n        # attn.shape == (batch_size, num_heads, seq_len, seq_len)\n        attn = F.softmax(attn, dim=-1)\n        y = torch.matmul(attn, v)\n        # y.shape == (batch_size, num_heads, seq_len, head_dim)\n        y = y.transpose(1, 2)\n        # y.shape == (batch_size, seq_len, num_heads, head_dim)\n        y = y.reshape(batch_size, seq_len, -1)\n        # y.shape == (batch_size, seq_len, embed_dim)\n        y = self.proj_dropout(self.proj(y))\n        return y","metadata":{"execution":{"iopub.status.busy":"2023-12-15T20:45:24.525746Z","iopub.execute_input":"2023-12-15T20:45:24.526262Z","iopub.status.idle":"2023-12-15T20:45:24.541306Z","shell.execute_reply.started":"2023-12-15T20:45:24.526224Z","shell.execute_reply":"2023-12-15T20:45:24.540367Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"**Testing and Verification:**","metadata":{}},{"cell_type":"code","source":"gpt_config = GPT1Config(vocab_size=10000, max_len=512, embed_dim=768, num_heads=12, num_blocks=12)\n\n# Create an instance of the GPT model\ngpt_model = GPT(gpt_config)\n\n# Generate a random input sequence for testing\ninput_sequence = torch.randint(0, gpt_config.vocab_size, (1, gpt_config.max_len))\n\n# Forward pass through the GPT model\noutput = gpt_model(input_sequence)\n\n# Print the output shape for verification\nprint(\"Output shape:\", output.shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T20:55:57.255019Z","iopub.execute_input":"2023-12-15T20:55:57.256570Z","iopub.status.idle":"2023-12-15T20:56:00.088361Z","shell.execute_reply.started":"2023-12-15T20:55:57.256518Z","shell.execute_reply":"2023-12-15T20:56:00.086794Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Output shape: torch.Size([1, 512, 10000])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The implementation is well-structured and aligns with the GPT-2 architecture. sample prediction demonstrate that the model is functional. However, further testing and evaluation metrics could enhance the validation process. As mentioned in assignment to abstrain using transformer libaray that's why directly GPT2model not imported directly for testing","metadata":{}}]}