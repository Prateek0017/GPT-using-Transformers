{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Task 3: Training Loop Implementation (40 Points)\n\n---\n\nFinally, create a training loop considering these following requirements:\n\n1. **Single GPU Training Loop:** Your base implementation should be equipped to train your model on a single GPU setup.\n2. **Distributed Data Parallel (DDP):** Extend your single GPU training loop to support training across multiple GPUs using DDP. Revisit the [PyTorch's DDP tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) for guidance.\n3. **Fully Sharded Data Parallel (FSDP):** Implement FSDP as a part of your training loop to shard the model parameters, gradients, and optimizer state. You can follow [Gupta et al., 2020, Training GPT-3 Like Models on a Single Machine](https://arxiv.org/pdf/2101.06840.pdf) for a comprehensive understanding of it.\n\n**Deliverable:** A Python script containing a functional training loop that is compatible with single GPU, DDP, and FSDP options along with a documentation illustrating how the code adapts to each setting.\n\n**Evaluation Scheme:** Each feature implementation will account for:\n\n- Single GPU: 10 points\n- DDP: 10 points\n- FSDP: 20 points\n\n**Note:** Document your code, approaches, difficulties encountered, and your solutions \nthoroughly. Include any reference materials you used in your report. Focus on clear communication of your methodologies and results.","metadata":{}},{"cell_type":"code","source":"import math\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nclass GPTConfig:\n    attn_dropout = 0.1\n    embed_dropout = 0.1\n    ff_dropout = 0.1\n    \n    def __init__(\n        self, vocab_size, max_len, **kwargs\n    ):\n        self.vocab_size = vocab_size\n        self.max_len = max_len\n        for key, value in kwargs.items():\n            setattr(self, key, value)\n\nclass GPT1Config(GPTConfig):\n    num_heads = 12\n    num_blocks = 12\n    embed_dim = 768","metadata":{"execution":{"iopub.status.busy":"2023-12-15T23:45:45.531311Z","iopub.execute_input":"2023-12-15T23:45:45.531683Z","iopub.status.idle":"2023-12-15T23:45:48.505354Z","shell.execute_reply.started":"2023-12-15T23:45:45.531651Z","shell.execute_reply":"2023-12-15T23:45:48.503641Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        embed_dim = config.embed_dim\n        self.max_len = config.max_len\n        self.tok_embed = nn.Embedding(\n            config.vocab_size, embed_dim\n        )\n        self.pos_embed = nn.Parameter(\n            torch.zeros(1, config.max_len, embed_dim)\n        )\n        self.dropout = nn.Dropout(config.embed_dropout)\n        self.blocks = nn.Sequential(\n            *[Block(config) for _ in range(config.num_blocks)]\n        )\n        self.ln = nn.LayerNorm(embed_dim)\n        self.fc = nn.Linear(embed_dim, config.vocab_size)\n    \n    def forward(self, x, target=None):\n        # batch_size = x.size(0)\n        seq_len = x.size(1)\n        assert seq_len <= self.max_len, \"sequence longer than model capacity\"\n        \n        tok_embedding = self.tok_embed(x)\n        # tok_embedding.shape == (batch_size, seq_len, embed_dim)\n        pos_embedding = self.pos_embed[:, :seq_len, :]\n        # pos_embedding.shape == (1, seq_len, embed_dim)\n        x = self.dropout(tok_embedding + pos_embedding)\n        x = self.blocks(x)\n        x = self.ln(x)\n        x = self.fc(x)\n        # x.shape == (batch_size, seq_len, vocab_size)\n        return x\n    \nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        embed_dim = config.embed_dim\n        self.ln1 = nn.LayerNorm(embed_dim)\n        self.ln2 = nn.LayerNorm(embed_dim)\n        self.attn = MultiheadAttention(config)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim * 4),\n            nn.GELU(),\n            nn.Linear(embed_dim * 4, embed_dim),\n            nn.Dropout(config.ff_dropout),\n        )\n    \n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))\n        x = x + self.ff(self.ln2(x))\n        return x\n    \n    \nclass MultiheadAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        embed_dim = config.embed_dim\n        self.num_heads = config.num_heads\n        assert embed_dim % self.num_heads == 0, \"invalid heads and embedding dimension configuration\"\n        self.key = nn.Linear(embed_dim, embed_dim)\n        self.value = nn.Linear(embed_dim, embed_dim)\n        self.query = nn.Linear(embed_dim, embed_dim)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.attn_dropout = nn.Dropout(config.attn_dropout)\n        self.proj_dropout = nn.Dropout(config.ff_dropout)\n        self.register_buffer(\n            \"mask\", \n            torch.tril(torch.ones(config.max_len, config.max_len))\n            .unsqueeze(0).unsqueeze(0)\n        )\n    \n    def forward(self, x):\n        batch_size = x.size(0)\n        seq_len = x.size(1)\n        # x.shape == (batch_size, seq_len, embed_dim)\n        k_t = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n        # shape == (batch_size, num_heads, seq_len, head_dim)\n        \n        attn = torch.matmul(q, k_t) / math.sqrt(q.size(-1))\n        # attn.shape == (batch_size, num_heads, seq_len, seq_len)\n        mask = self.mask[:, :, :seq_len, :seq_len]\n        attn = attn.masked_fill(mask == 0, float(\"-inf\"))\n        attn = self.attn_dropout(attn)\n        # attn.shape == (batch_size, num_heads, seq_len, seq_len)\n        attn = F.softmax(attn, dim=-1)\n        y = torch.matmul(attn, v)\n        # y.shape == (batch_size, num_heads, seq_len, head_dim)\n        y = y.transpose(1, 2)\n        # y.shape == (batch_size, seq_len, num_heads, head_dim)\n        y = y.reshape(batch_size, seq_len, -1)\n        # y.shape == (batch_size, seq_len, embed_dim)\n        y = self.proj_dropout(self.proj(y))\n        return y\n    \ngpt_config = GPT1Config(vocab_size=10000, max_len=512, embed_dim=768, num_heads=12, num_blocks=12)\n\n# Create an instance of the GPT model\ngpt_model = GPT(gpt_config)\n\n# Generate a random input sequence for testing\ninput_sequence = torch.randint(0, gpt_config.vocab_size, (1, gpt_config.max_len))\n\n# Forward pass through the GPT model\noutput = gpt_model(input_sequence)\n\n# Print the output shape for verification\nprint(\"Output shape:\", output.shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T23:45:50.673941Z","iopub.execute_input":"2023-12-15T23:45:50.674442Z","iopub.status.idle":"2023-12-15T23:45:52.975432Z","shell.execute_reply.started":"2023-12-15T23:45:50.674408Z","shell.execute_reply":"2023-12-15T23:45:52.973893Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Output shape: torch.Size([1, 512, 10000])\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Task 3 solution**\n\n**Single GPU Training Loop:**\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Assuming have a random dataset\nnum_epochs = 10 \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(gpt_model.parameters(), lr=0.001) #gpt_model is defined in task 1\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngpt_model.to(device)\n\n# Generate a random dataset for illustration\ndata_size = 1000\nseq_length = 512\nvocab_size = 10000\n\n# Random input sequences\ninputs = torch.randint(0, vocab_size, (data_size, seq_length))\n# Corresponding random target sequences with the same length as inputs\ntargets = torch.randint(0, vocab_size, (data_size, seq_length))\n\n# Create a TensorDataset from inputs and targets\ndataset = TensorDataset(inputs, targets)\n\n# Define batch size\nbatch_size = 32\n\n# Create a DataLoader\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n\n# Training loop\nfor epoch in range(num_epochs):\n    gpt_model.train()\n    for batch in dataloader:\n        inputs, targets = batch\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = gpt_model(inputs)\n        \n        # Make sure targets have the same sequence length as outputs\n        targets = targets[:, :outputs.size(1)]  \n\n        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n\n        loss.backward()\n        optimizer.step()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-16T00:03:38.841000Z","iopub.execute_input":"2023-12-16T00:03:38.841342Z","iopub.status.idle":"2023-12-16T00:03:49.645181Z","shell.execute_reply.started":"2023-12-16T00:03:38.841314Z","shell.execute_reply":"2023-12-16T00:03:49.643194Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Make sure targets have the same sequence length as outputs\u001b[39;00m\n\u001b[1;32m     51\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets[:, :outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)]  \n\u001b[0;32m---> 53\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     56\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mValueError\u001b[0m: Expected input batch_size (5120000) to match target batch_size (5120)."],"ename":"ValueError","evalue":"Expected input batch_size (5120000) to match target batch_size (5120).","output_type":"error"}]},{"cell_type":"markdown","source":"**Distributed Data Parallel (DDP):**\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.utils.data import DataLoader\n\n# Assuming DistributedSampler and DataLoader as it takes to much ram to run\n\n# Initialize the model on each GPU\nmodel = GPT(gpt_config).to(device)\nmodel = DistributedDataParallel(model)\n\n# Defining loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    for batch in dataloader:\n        inputs, targets = batch\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n\n        loss.backward()\n        optimizer.step()\n\n# Save the trained model\nif torch.distributed.get_rank() == 0:\n    torch.save(model.module.state_dict(), 'ddp_model.pth')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Fully Sharded Data Parallel (FSDP):**\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nfrom torch.distributed.fsdp import FullyShardedDataParallel \n\n# Assuming set up FSDP and DataLoader as it takes too much space to run\n\n# Initialize the model on each GPU\nmodel = GPT(gpt_config).cuda()\nfsdp_model = FullyShardedDataParallel(model)\n\n# Definingloss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(fsdp_model.parameters(), lr=0.001)\n\n# Training loop\nfor epoch in range(num_epochs):\n    fsdp_model.train()\n    for batch in dataloader:\n        inputs, targets = batch\n        inputs, targets = inputs.cuda(), targets.cuda()\n\n        optimizer.zero_grad()\n\n        outputs = fsdp_model(inputs)\n        loss = criterion(outputs, targets)\n\n        loss.backward()\n        optimizer.step()\n\n# Save the trained model\ntorch.save(fsdp_model.module.state_dict(), 'fsdp_model.pth')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-16T00:06:42.324572Z","iopub.execute_input":"2023-12-16T00:06:42.325099Z","iopub.status.idle":"2023-12-16T00:06:43.716722Z","shell.execute_reply.started":"2023-12-16T00:06:42.325052Z","shell.execute_reply":"2023-12-16T00:06:43.715117Z"},"trusted":true},"execution_count":10,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FullyShardedDataParallel \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming set up FSDP and DataLoader as it takes too much space to run\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize the model on each GPU\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt_config\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m fsdp_model \u001b[38;5;241m=\u001b[39m FullyShardedDataParallel(model)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Definingloss function and optimizer\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"],"ename":"AssertionError","evalue":"Torch not compiled with CUDA enabled","output_type":"error"}]},{"cell_type":"markdown","source":"There three are the template codes for \n- Single GPU\n- DDP\n- FSDP\n\nThese templates can be used with GPT model made in task 1 with gpu.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}